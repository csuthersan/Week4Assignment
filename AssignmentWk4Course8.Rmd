---
title: "AssignmentWk4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Reading Data



```{r}
setwd("/Users/chen/downloads")
training <- read.csv('pml-training.csv')
testing <- read.csv('pml-testing.csv')
```

## Split data into training, testing and validataion


```{r}
library(caret)
set.seed(200)

inTrain = createDataPartition(training$classe, p = 0.8, list = FALSE)
test <- training[-inTrain,]
Tr <- training[inTrain,]
intrain2 = createDataPartition(Tr$classe, p = 0.75, list = FALSE)
training2 <- Tr[intrain2,]
valid <- Tr[-intrain2,]
```

## Inspect Data
```{r, echo=TRUE}
str(training2)
# I can see many factor variables are in fact numerical. 
# Looking at these columns I can also see that they have a lot of missing data. I will remove them
summary(training2$classe)
```

## Removing columns without much data
```{r, results = 'hide', echo=TRUE}
lose_col <- c('X','user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 
          'new_window', ' kurtosis_roll_belt', 'kurtosis_picth_belt', 'kurtosis_yaw_belt',
          'skewness_roll_belt', 'skewness_roll_belt.1', ' skewness_yaw_belt', 'max_yaw_belt',
          'min_yaw_belt', 'amplitude_yaw_belt', 'kurtosis_roll_arm', 'kurtosis_picth_arm', 
          'kurtosis_yaw_arm', 'skewness_roll_arm', 'skewness_pitch_arm', 'skewness_yaw_arm',
          'kurtosis_roll_dumbbell', 'kurtosis_picth_dumbbell', 'kurtosis_yaw_dumbbell', 
          'skewness_roll_dumbbell', 'skewness_pitch_dumbbell', 'skewness_yaw_dumbbell',
          'max_yaw_dumbbell', 'min_yaw_dumbbell', 'amplitude_yaw_dumbbell', 'yaw_forearm',
          'kurtosis_roll_forearm', 'kurtosis_picth_forearm', 'kurtosis_yaw_forearm', 
          'skewness_roll_forearm', 'skewness_pitch_forearm', 'skewness_yaw_forearm', 'max_yaw_forearm',
          'min_yaw_forearm' ,'amplitude_yaw_forearm', 'skewness_yaw_belt', 'kurtosis_roll_belt')


#Inspecting some variables with a high amount of missing values to see if i should remove

table(training2$classe, training2$new_window)
table(training2$classe, training2$kurtosis_roll_belt)
table(training2$classe, training2$kurtosis_yaw_belt)
#None of these columns had any relationship with training2$classe and it does not look like i will lose a lot of explantory power by excluding them

#Removing columns

training2 <- training2[ , !(names(training2) %in% lose_col)]
```

## Look to see what data looks like now

```{r, echo=TRUE}
str(training2)
```
There are still a lot of columns without much data!

##Remove columns with a lot of NA's and look at data
```{r, echo=TRUE}
colsToDel = names(training2)[colMeans(is.na(training2))>0.9]
training2 <- training2[ , !(names(training2) %in% colsToDel)]
str(training2)
```
Finally it looks ready to use

## 1 Fit RF model
As it is a randomForest, I don't need to scale and center pre-process.
```{r, echo=TRUE}
mod1 <- train(classe ~., method= 'rf', data = training2,na.action = na.exclude)
predrf <- predict(mod1, test)
test$rfright <- test$classe == predrf
mean(test$rfright)
```
I was planning on doing an ensemble model but the results were so good, 99.6% 
accuracy on test that i will just stay with the random forest
#The Outer Sample accuract is thus 99.6%

## Test on validation set
As said earlier, I was planning on using an ensemble model so i set aside a validation set
I suppose I do not need to apply the model on validation to get an out of sample error estimate
but i did it anyway to confirm my earlier estimate of 99.6%
```{r, echo=TRUE}
predrf2 <- predict(mod1, valid)
valid$rfright <- valid$classe == predrf2
mean(valid$rfright)
```
Also very high accuracy at 99.7% and confirms my earlier out of sample estimate.

## answers for the quiz
```{r, echo=TRUE}
predrf3 <- predict(mod1, testing)
predrf3
```